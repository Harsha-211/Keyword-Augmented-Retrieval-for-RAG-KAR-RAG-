{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ffb8773-fdcf-4b64-b3b3-d958ff7c04fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, DataCollatorForSeq2Seq\n",
    "from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments,Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "265928d1-20b4-4b80-b425-756fcfa31928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a4ca96e-b787-4622-851c-f2f75158bfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"json\",data_files=\"D:/Pythonn/Optimizing VectorDB/data.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc29dcb5-2f0a-479f-ad1a-43e84f1a03ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[\"train\"].train_test_split(test_size=0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e7a751b-7be6-49a9-acc3-025d0e0c9d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'completion'],\n",
       "        num_rows: 2265\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'completion'],\n",
       "        num_rows: 567\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d6181b9-58fc-4a75-813e-32cdd43c78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(examples):\n",
    "    inputs = tokenizer(examples[\"prompt\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "    labels = tokenizer(examples[\"completion\"], truncation=True, padding=\"max_length\", max_length=64)\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6327f40a-c4a8-468c-90bb-959ceb47227a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13df4f2b58c4e94ac9e0492294a5f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2265 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236887d2995d47238d3abc2205dce003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/567 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = data.map(preprocess,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84c21611-12ca-4bf7-b5ef-14e02ac5fb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\",quiet=True)\n",
    "nltk.download('punkt_tab')\n",
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46a25ee3-c895-4937-bc31-0b022f0c333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "  \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c02bb428-d95e-4c6b-894b-b76f675440b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_RATE = 3e-4\n",
    "BATCH_SIZE = 8\n",
    "PER_DEVICE_EVAL_BATCH = 4\n",
    "WEIGHT_DECAY = 0.01\n",
    "SAVE_TOTAL_LIM = 3\n",
    "NUM_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22d9f464-811a-43b3-abb7-92fd53d05601",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "   output_dir=\"./results\",\n",
    "   eval_strategy=\"epoch\",\n",
    "   learning_rate=L_RATE,\n",
    "   per_device_train_batch_size=BATCH_SIZE,\n",
    "   per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n",
    "   weight_decay=WEIGHT_DECAY,\n",
    "   save_total_limit=SAVE_TOTAL_LIM,\n",
    "   num_train_epochs=NUM_EPOCHS,\n",
    "   predict_with_generate=True,\n",
    "   push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4de5ce2b-9152-4262-99bd-660cf4042d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\AppData\\Local\\Temp\\ipykernel_13276\\2158542907.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset= tokenized_dataset[\"train\"],\n",
    "    eval_dataset = tokenized_dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "faa800fe-eb5e-4369-a288-6f8082f2e9e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='852' max='852' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [852/852 42:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.112118</td>\n",
       "      <td>0.585331</td>\n",
       "      <td>0.568793</td>\n",
       "      <td>0.585488</td>\n",
       "      <td>0.585266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.065052</td>\n",
       "      <td>0.585299</td>\n",
       "      <td>0.568944</td>\n",
       "      <td>0.585399</td>\n",
       "      <td>0.585147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.067827</td>\n",
       "      <td>0.585438</td>\n",
       "      <td>0.569050</td>\n",
       "      <td>0.585491</td>\n",
       "      <td>0.585293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=852, training_loss=0.021277781383532313, metrics={'train_runtime': 2523.7575, 'train_samples_per_second': 2.692, 'train_steps_per_second': 0.338, 'total_flos': 4652926209884160.0, 'train_loss': 0.021277781383532313, 'epoch': 3.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9edb7706-486f-451a-b09e-1a0291c685c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_checkpoint = \"D:/Pythonn/Optimizing VectorDB/results/checkpoint-852\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2b4bc30-d2e0-4ce4-abae-1d619812558c",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = T5ForConditionalGeneration.from_pretrained(last_checkpoint)\n",
    "tokenizer = T5Tokenizer.from_pretrained(last_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "015ff6b5-7816-4a9a-b28e-1fc681022ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragarph = \"Self-driving cars rely on a combination of sensors, cameras, radar, and artificial intelligence to navigate roads without human input. These autonomous vehicles use real-time data to detect obstacles, interpret traffic signs, and make split-second decisions. Advances in deep learning and computer vision have significantly improved their ability to operate safely in complex environments.\"\n",
    "inputs = f\"Etract Keywords from the Paragraph:\\n {paragarph}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a710015e-19ce-40f7-88d9-c28d70fe5a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>Self-driving cars rely combination sensors cameras radar artificial intelligence navigate roads without human input autonomous'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(inputs, return_tensors=\"pt\")\n",
    "outputs = finetuned_model.generate(**inputs)\n",
    "answer = tokenizer.decode(outputs[0])\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c7ab3f-ad1d-4c1b-9883-b571250e67f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
